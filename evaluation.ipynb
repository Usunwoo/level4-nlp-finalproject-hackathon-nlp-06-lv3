{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result 평가 스크립트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_SUMMARY_CSV = \"evaluation/data/generated_summary.csv\"\n",
    "GENERATED_CATEGORY_CSV = \"evaluation/data/generated_category.csv\"\n",
    "GENERATED_REPORT_CSV = \"evaluation/data/generated_report.csv\"\n",
    "\n",
    "REFERNECE_CSV = \"evaluation/data/reference.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import 및 환경 주입\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from agents.classification.classification_type import ClassificationType\n",
    "from evaluation.classification.dataframe_manager import DataFrameManager\n",
    "from evaluation.evaluation_summary import evaluate_summary\n",
    "from evaluation.gpt_eval import calculate_g_eval\n",
    "from evaluation.result_printer import print_evaluation_results\n",
    "from utils.configuration import Config\n",
    "\n",
    "load_dotenv()\n",
    "Config.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(reference_df): 48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>subject</th>\n",
       "      <th>category</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194e90265a3c53fe</td>\n",
       "      <td>*지속가능원이 추천하는 2025-1 지속가능 교과목 25선!*\\n\\nESG? 지속가...</td>\n",
       "      <td>[지속가능원] 지속가능원이 추천하는 2025-1 지속가능 교과목 25선!</td>\n",
       "      <td>other</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194e343333e5ff27</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>ICT명품인재양성사업단 뉴스레터 겨울호를 보내 드립니다.</td>\n",
       "      <td>academic</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194e34039181a3c0</td>\n",
       "      <td>안녕하세요,\\n\\n\\n2024년 업적평가를 위해 실적 업데이트 요청을 드립니다. 이...</td>\n",
       "      <td>2024년 업적평가를 위한 연구실적 업데이트 요청의 건 (~2월 7일)</td>\n",
       "      <td>academic</td>\n",
       "      <td>action needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194e334940f6bad0</td>\n",
       "      <td>안녕하세요\\n친환경 디지털 정보과학 교육연구단 함이열입니다.\\n\\nBK21 대학원혁...</td>\n",
       "      <td>[BK] 2025년 2월 7일(금) BK21 대학원혁신사업 영어논문작성법 워크숍 안...</td>\n",
       "      <td>academic</td>\n",
       "      <td>action needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194e325d62686180</td>\n",
       "      <td>Hi there,\\n\\nWe charged $5.20 to your credit c...</td>\n",
       "      <td>Your OpenAI API account has been funded</td>\n",
       "      <td>other</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                               body  \\\n",
       "0  194e90265a3c53fe  *지속가능원이 추천하는 2025-1 지속가능 교과목 25선!*\\n\\nESG? 지속가...   \n",
       "1  194e343333e5ff27                         \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "2  194e34039181a3c0  안녕하세요,\\n\\n\\n2024년 업적평가를 위해 실적 업데이트 요청을 드립니다. 이...   \n",
       "3  194e334940f6bad0  안녕하세요\\n친환경 디지털 정보과학 교육연구단 함이열입니다.\\n\\nBK21 대학원혁...   \n",
       "4  194e325d62686180  Hi there,\\n\\nWe charged $5.20 to your credit c...   \n",
       "\n",
       "                                             subject  category         action  \n",
       "0           [지속가능원] 지속가능원이 추천하는 2025-1 지속가능 교과목 25선!     other      read only  \n",
       "1                    ICT명품인재양성사업단 뉴스레터 겨울호를 보내 드립니다.  academic      read only  \n",
       "2            2024년 업적평가를 위한 연구실적 업데이트 요청의 건 (~2월 7일)  academic  action needed  \n",
       "3  [BK] 2025년 2월 7일(금) BK21 대학원혁신사업 영어논문작성법 워크숍 안...  academic  action needed  \n",
       "4            Your OpenAI API account has been funded     other      read only  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_df = pd.read_csv(REFERNECE_CSV)\n",
    "print(\"len(reference_df):\", len(reference_df))\n",
    "reference_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary 평가\n",
    "\n",
    "1. 요약전 원문(고정)\n",
    "2. REFERENCE(고정)\n",
    "3. 생성 요약문\n",
    "\n",
    "Return\n",
    "ROUGE, BERT SCORE, G-EVAL(with summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(generated_summary_df): 48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194e90265a3c53fe</td>\n",
       "      <td>지속가능원이 추천하는 2025-1 학기 지속가능 교과목 25선 안내, 링크를 통해 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194e343333e5ff27</td>\n",
       "      <td>ICT명품인재양성사업단 뉴스레터 겨울호를 보내 드립니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194e34039181a3c0</td>\n",
       "      <td>2024년 업적평가를 위한 연구실적 업데이트 요청, 논문, 학회, 특허, 연구비 실...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194e334940f6bad0</td>\n",
       "      <td>BK21 대학원혁신사업 2025년 1학기 대학원 맞춤형 연구역량 프로그램 - 영어논...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194e325d62686180</td>\n",
       "      <td>신용카드(끝자리 2043)로 5.20달러가 청구되어 OpenAI API 크레딧 잔액...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                            summary\n",
       "0  194e90265a3c53fe  지속가능원이 추천하는 2025-1 학기 지속가능 교과목 25선 안내, 링크를 통해 ...\n",
       "1  194e343333e5ff27                    ICT명품인재양성사업단 뉴스레터 겨울호를 보내 드립니다.\n",
       "2  194e34039181a3c0  2024년 업적평가를 위한 연구실적 업데이트 요청, 논문, 학회, 특허, 연구비 실...\n",
       "3  194e334940f6bad0  BK21 대학원혁신사업 2025년 1학기 대학원 맞춤형 연구역량 프로그램 - 영어논...\n",
       "4  194e325d62686180  신용카드(끝자리 2043)로 5.20달러가 청구되어 OpenAI API 크레딧 잔액..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_summary_df = pd.read_csv(GENERATED_SUMMARY_CSV)\n",
    "print(\"len(generated_summary_df):\", len(generated_summary_df))\n",
    "generated_summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "g-eval start with **gpt-4**\n",
      "\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=The text is not provided, so I cannot evaluate the consistency of the summary.\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=3\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4.5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4.5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=3\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=3\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=2\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=The provided text does not allow me to evaluate the relevance of the summary as the source text and the summary are in different languages. I am unable to compare the summary to the email and identify the key points, essential details, and main purpose of the\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=2\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=3\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=3\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=4\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n"
     ]
    }
   ],
   "source": [
    "source_texts = reference_df[\"body\"].tolist()\n",
    "report_texts = generated_summary_df[\"summary\"].tolist()\n",
    "reference_texts = reference_df[\"subject\"].tolist()\n",
    "\n",
    "\n",
    "summary_results = evaluate_summary(source_texts, report_texts, reference_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SUMMARY Evaluation Results =====\n",
      "\n",
      "--- Summary Sample 1 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:0.9323, R:0.9488, F:0.9404\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 2 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=0.0000, coherence=1.0000, fluency=1.0000, relevance=3.0000\n",
      "\n",
      "--- Summary Sample 3 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:0.9181, R:0.9636, F:0.9403\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 4 ---\n",
      "[ROUGE] R1=(P:0.3333,R:0.8000,F:0.4706), R2=(P:0.0909,R:0.2500,F:0.1333), RL=(P:0.2500,R:0.6000,F:0.3529)\n",
      "[BERT] P:0.9066, R:0.9386, F:0.9223\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 5 ---\n",
      "[ROUGE] R1=(P:0.4000,R:0.2857,F:0.3333), R2=(P:0.2500,R:0.1667,F:0.2000), RL=(P:0.4000,R:0.2857,F:0.3333)\n",
      "[BERT] P:0.5310, R:0.6665, F:0.5911\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 6 ---\n",
      "[ROUGE] R1=(P:0.4000,R:0.2857,F:0.3333), R2=(P:0.2500,R:0.1667,F:0.2000), RL=(P:0.4000,R:0.2857,F:0.3333)\n",
      "[BERT] P:0.5322, R:0.6655, F:0.5914\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 7 ---\n",
      "[ROUGE] R1=(P:0.4000,R:0.2857,F:0.3333), R2=(P:0.2500,R:0.1667,F:0.2000), RL=(P:0.4000,R:0.2857,F:0.3333)\n",
      "[BERT] P:0.5325, R:0.6652, F:0.5915\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 8 ---\n",
      "[ROUGE] R1=(P:0.2000,R:0.3333,F:0.2500), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.2000,R:0.3333,F:0.2500)\n",
      "[BERT] P:0.8000, R:0.8679, F:0.8326\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "--- Summary Sample 9 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.6850, R:0.8665, F:0.7651\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 10 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.7745, R:0.8638, F:0.8167\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 11 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.7976, R:0.9067, F:0.8487\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 12 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.7976, R:0.9067, F:0.8487\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 13 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8175, R:0.8930, F:0.8536\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 14 ---\n",
      "[ROUGE] R1=(P:0.2000,R:1.0000,F:0.3333), R2=(P:0.1111,R:1.0000,F:0.2000), RL=(P:0.2000,R:1.0000,F:0.3333)\n",
      "[BERT] P:0.8199, R:0.8931, F:0.8549\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 15 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.5000,R:0.5000,F:0.5000), RL=(P:0.6667,R:0.6667,F:0.6667)\n",
      "[BERT] P:0.9147, R:0.9233, F:0.9190\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 16 ---\n",
      "[ROUGE] R1=(P:0.3333,R:1.0000,F:0.5000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.3333,R:1.0000,F:0.5000)\n",
      "[BERT] P:0.8772, R:0.9241, F:0.9000\n",
      "[G-EVAL] consistency=5.0000, coherence=2.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 17 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:0.9261, R:0.9671, F:0.9462\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 18 ---\n",
      "[ROUGE] R1=(P:0.0833,R:1.0000,F:0.1538), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0833,R:1.0000,F:0.1538)\n",
      "[BERT] P:0.8057, R:0.9040, F:0.8520\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 19 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8967, R:0.9152, F:0.9059\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 20 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8725, R:0.8960, F:0.8841\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 21 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.9159, R:0.9269, F:0.9214\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 22 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8153, R:0.8779, F:0.8455\n",
      "[G-EVAL] consistency=5.0000, coherence=2.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 23 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8580, R:0.8953, F:0.8763\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 24 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.9088, R:0.8725, F:0.8903\n",
      "[G-EVAL] consistency=1.0000, coherence=5.0000, fluency=1.0000, relevance=3.0000\n",
      "\n",
      "--- Summary Sample 25 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8850, R:0.9240, F:0.9041\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 26 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8443, R:0.8735, F:0.8587\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 27 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8923, R:0.8953, F:0.8938\n",
      "[G-EVAL] consistency=3.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 28 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8643, R:0.8746, F:0.8694\n",
      "[G-EVAL] consistency=5.0000, coherence=2.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 29 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8808, R:0.9245, F:0.9021\n",
      "[G-EVAL] consistency=2.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 30 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.9215, R:0.9144, F:0.9179\n",
      "[G-EVAL] consistency=1.0000, coherence=4.0000, fluency=1.0000, relevance=0.0000\n",
      "\n",
      "--- Summary Sample 31 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8856, R:0.9232, F:0.9040\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 32 ---\n",
      "[ROUGE] R1=(P:0.1333,R:0.2500,F:0.1739), R2=(P:0.0714,R:0.1429,F:0.0952), RL=(P:0.1333,R:0.2500,F:0.1739)\n",
      "[BERT] P:0.6008, R:0.7306, F:0.6593\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=2.0000\n",
      "\n",
      "--- Summary Sample 33 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8141, R:0.8469, F:0.8302\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 34 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.7734, R:0.8669, F:0.8175\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 35 ---\n",
      "[ROUGE] R1=(P:0.0400,R:1.0000,F:0.0769), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0400,R:1.0000,F:0.0769)\n",
      "[BERT] P:0.7730, R:0.8839, F:0.8248\n",
      "[G-EVAL] consistency=5.0000, coherence=1.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 36 ---\n",
      "[ROUGE] R1=(P:0.3636,R:1.0000,F:0.5333), R2=(P:0.2000,R:0.6667,F:0.3077), RL=(P:0.3636,R:1.0000,F:0.5333)\n",
      "[BERT] P:0.8485, R:0.8967, F:0.8719\n",
      "[G-EVAL] consistency=3.0000, coherence=5.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "--- Summary Sample 37 ---\n",
      "[ROUGE] R1=(P:0.5000,R:0.5000,F:0.5000), R2=(P:0.2857,R:0.2857,F:0.2857), RL=(P:0.5000,R:0.5000,F:0.5000)\n",
      "[BERT] P:0.8722, R:0.8885, F:0.8803\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "--- Summary Sample 38 ---\n",
      "[ROUGE] R1=(P:0.2222,R:1.0000,F:0.3636), R2=(P:0.1250,R:1.0000,F:0.2222), RL=(P:0.2222,R:1.0000,F:0.3636)\n",
      "[BERT] P:0.8569, R:0.9247, F:0.8895\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 39 ---\n",
      "[ROUGE] R1=(P:0.4000,R:1.0000,F:0.5714), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.2000,R:0.5000,F:0.2857)\n",
      "[BERT] P:0.8275, R:0.8779, F:0.8520\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 40 ---\n",
      "[ROUGE] R1=(P:0.2000,R:1.0000,F:0.3333), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.2000,R:1.0000,F:0.3333)\n",
      "[BERT] P:0.9000, R:0.9269, F:0.9133\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 41 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:0.8854, R:0.9282, F:0.9063\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 42 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8443, R:0.9571, F:0.8972\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 43 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8662, R:0.9289, F:0.8964\n",
      "[G-EVAL] consistency=3.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 44 ---\n",
      "[ROUGE] R1=(P:1.0000,R:0.5000,F:0.6667), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:0.5000,F:0.6667)\n",
      "[BERT] P:0.9034, R:0.9197, F:0.9114\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 45 ---\n",
      "[ROUGE] R1=(P:0.1364,R:0.6000,F:0.2222), R2=(P:0.0952,R:0.5000,F:0.1600), RL=(P:0.1364,R:0.6000,F:0.2222)\n",
      "[BERT] P:0.5453, R:0.8059, F:0.6505\n",
      "[G-EVAL] consistency=5.0000, coherence=2.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 46 ---\n",
      "[ROUGE] R1=(P:0.0870,R:0.2000,F:0.1212), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0870,R:0.2000,F:0.1212)\n",
      "[BERT] P:0.5253, R:0.6833, F:0.5940\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 47 ---\n",
      "[ROUGE] R1=(P:0.7143,R:1.0000,F:0.8333), R2=(P:0.5000,R:0.7500,F:0.6000), RL=(P:0.7143,R:1.0000,F:0.8333)\n",
      "[BERT] P:0.8526, R:0.8617, F:0.8572\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 48 ---\n",
      "[ROUGE] R1=(P:0.5556,R:0.8333,F:0.6667), R2=(P:0.3750,R:0.6000,F:0.4615), RL=(P:0.5556,R:0.8333,F:0.6667)\n",
      "[BERT] P:0.8167, R:0.8802, F:0.8473\n",
      "[G-EVAL] consistency=4.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "===== Averages =====\n",
      "\n",
      "[ROUGE Avg]\n",
      "  ROUGE-1: P=0.2646, R=0.1707, F1=0.3925\n",
      "  ROUGE-2: P=0.0000, R=0.0000, F1=0.0000\n",
      "  ROUGE-L: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "[BERT Avg]\n",
      "  Precision: 0.8191, Recall: 0.8810, F1: 0.8476\n",
      "\n",
      "[G-EVAL Avg]\n",
      "  consistency=4.3542, coherence=4.1667, fluency=1.0000, relevance=4.6042\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(summary_results, eval_type=\"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류 평가\n",
    "\n",
    "1. 라벨 Ground truth(고정)\n",
    "2. 여러번 결과(N번) 가지고 있는 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(generated_category_df): 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>category4</th>\n",
       "      <th>category5</th>\n",
       "      <th>action1</th>\n",
       "      <th>action2</th>\n",
       "      <th>action3</th>\n",
       "      <th>action4</th>\n",
       "      <th>action5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194e90265a3c53fe</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194e34039181a3c0</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id category1 category2 category3 category4 category5  \\\n",
       "0  194e90265a3c53fe     other     other     other     other     other   \n",
       "1  194e34039181a3c0  academic  academic  academic  academic  academic   \n",
       "\n",
       "         action1        action2        action3        action4        action5  \n",
       "0      read only      read only      read only      read only      read only  \n",
       "1  action needed  action needed  action needed  action needed  action needed  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_category_df = pd.read_csv(GENERATED_CATEGORY_CSV)\n",
    "print(\"len(generated_category_df):\", len(generated_category_df))\n",
    "generated_category_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Confusion Matrix 저장 완료: evaluation/classification/figure\\academic_confusion_matrix.png\n",
      "✅ Confusion Matrix 저장 완료: evaluation/classification/figure\\other_confusion_matrix.png\n",
      "\n",
      "Correctness\n",
      "🎯 전체 정확도: 1.0000\n",
      "🎯 academic 정확도: 1.0000\n",
      "🎯 other 정확도: 1.0000\n",
      "\n",
      "Consistency\n",
      "📊 Ground Truth 별 요약된 평가 메트릭\n",
      "  Ground Truth  Entropy  Diversity Index  Chi-Square p-value  Accuracy  \\\n",
      "0     academic      0.0              0.2                 1.0       1.0   \n",
      "1        other      0.0              0.2                 1.0       1.0   \n",
      "\n",
      "   Cramer's V  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "\n",
      "=== Overall Multiclass Confusion Matrix ===\n",
      "Labels: ['academic', 'other']\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "===========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "category_df_manager = DataFrameManager(5, ClassificationType.CATEGORY)\n",
    "for index, row in generated_category_df.iterrows():\n",
    "    results = [row[\"category1\"], row[\"category2\"], row[\"category3\"], row[\"category4\"], row[\"category5\"]]\n",
    "    category_df_manager.update_eval_df(row[\"id\"], results, reference_df.loc[index, \"category\"])\n",
    "\n",
    "category_df_manager.print_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Confusion Matrix 저장 완료: evaluation/classification/figure\\read only_confusion_matrix.png\n",
      "\n",
      "Correctness\n",
      "🎯 전체 정확도: 0.5000\n",
      "🎯 read only 정확도: 0.5000\n",
      "\n",
      "Consistency\n",
      "📊 Ground Truth 별 요약된 평가 메트릭\n",
      "  Ground Truth   Entropy  Diversity Index  Chi-Square p-value  Accuracy  \\\n",
      "0    read only  0.693147              0.2                   1       0.5   \n",
      "\n",
      "   Cramer's V  \n",
      "0         0.0  \n",
      "\n",
      "=== Overall Multiclass Confusion Matrix ===\n",
      "Labels: ['action needed', 'read only']\n",
      "[[0 0]\n",
      " [5 5]]\n",
      "===========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "action_df_manager = DataFrameManager(5, ClassificationType.ACTION)\n",
    "for index, row in generated_category_df.iterrows():\n",
    "    results = [row[\"action1\"], row[\"action2\"], row[\"action3\"], row[\"action4\"], row[\"action5\"]]\n",
    "    action_df_manager.update_eval_df(row[\"id\"], results, reference_df.loc[index, \"action\"])\n",
    "\n",
    "action_df_manager.print_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 report 평가\n",
    "\n",
    "1. 요약전 원문(메일 요약문 concat)\n",
    "2. 생성 요약문\n",
    "\n",
    "Return\n",
    "G-EVAL(with final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(generated_report_df): 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. 지속가능원이 추천하는 2025-1 학기 지속가능 교과목 25선 안내.\\r\\n2...</td>\n",
       "      <td>1. 지속가능원이 추천하는 2025-1 학기 지속가능 교과목 25선 안내.\\r\\n2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  1. 지속가능원이 추천하는 2025-1 학기 지속가능 교과목 25선 안내.\\r\\n2...   \n",
       "\n",
       "                                              report  \n",
       "0  1. 지속가능원이 추천하는 2025-1 학기 지속가능 교과목 25선 안내.\\r\\n2...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_report_df = pd.read_csv(GENERATED_REPORT_CSV)\n",
    "print(\"len(generated_report_df):\", len(generated_report_df))\n",
    "generated_report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "g-eval start with **gpt-4**\n",
      "\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=4\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "results[\"g-eval\"] = calculate_g_eval(\n",
    "    source_texts=generated_report_df[\"source\"].tolist(),\n",
    "    generated_texts=generated_report_df[\"report\"].tolist(),\n",
    "    eval_type=\"report\",\n",
    "    model_name=Config.config[\"summary\"][\"g_eval\"][\"openai_model\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== REPORT Evaluation Results =====\n",
      "\n",
      "--- Report Sample 1 ---\n",
      "[G-EVAL] consistency=4.0000, coherence=4.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "===== Averages =====\n",
      "\n",
      "[G-EVAL Avg]\n",
      "  consistency=4.0000, coherence=4.0000, fluency=1.0000, relevance=4.0000\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(results, eval_type=\"report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
