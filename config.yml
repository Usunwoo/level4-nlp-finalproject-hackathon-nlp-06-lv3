# TODO: config 중복 코드 삭제할 것

gmail:
  # gmail에서 불러올 시작 날짜 (값이 없는 경우 2025/01/10)
  start_date:
  # gmail에서 불러올 끝 날짜 (값이 없는 경우 오늘 날짜)
  end_date:
  # gmail에서 불러올 메일 최대 개수
  max_mails: 10 # TODO: 10으로 원상복구

# 평가 설정
evaluation:
  summary_eval: false # Summary 평가 수행 여부
  classification_eval: false # Classification 평가 수행 여부
  report_eval: false # Final Report 평가 수행 여부

  solar_as_judge:
    items:
      item1: "요약문에 원문 이메일의 목적이 잘 드러났는지?"
      item2: "이메일 원문에 일정 사항이 명시 돼 있다면, 요약문에 명시된 일정 사항이 잘 드러나있는지? (이메일의 원문에 일정 사항이 애초에 없다면 무조건 Yes로 답할 것)"
      item3: "이메일 원문에 요청 사항이 명시 돼 있다면, 요약문에 명시된 요청 사항이 잘 드러나있는지? (이메일의 원문에 요청 사항이 애초에 없다면 무조건 Yes로 답할 것)"
      item4: "요약문이 한국어로 작성 됐는지?"
      item5: "요약문이 명사형 어미로 작성 됐는지?"
    prompt_path: prompt/template/solar_as_judge/solar_as_judge_prompt_template.txt
    items_in_a_volume: 3

seed: 42
temperature:
  summary: 0
  classification: 0.7

self_reflection:
  type: self-refine # self-refine | reflexion 변경 가능
  max_iteration: 3 # TODO: 3으로 원상복구
  reflexion:
    threshold_type: "average"
    threshold: 3.5

# Summary 평가 관련 설정
summary:
  metrics:
    - rouge
    - bert
    - g-eval

  bert_model: "distilbert-base-uncased"

  g_eval:
    openai_model: "gpt-4" # summary는 gpt-4가 아니면 정확한 답변 생성이 어려움
    additional: False # "readability", "clearance", "practicality"를 G-Eval에 적용할 여부
    prompts:
      consistency: "prompt/template/g_eval/con_summary.txt"
      coherence: "prompt/template/g_eval/coh_summary.txt"
      fluency: "prompt/template/g_eval/flu_summary.txt"
      relevance: "prompt/template/g_eval/rel_summary.txt"
      # readability: "prompt/template/g_eval/rdb_summary.txt"
      # clearance: "prompt/template/g_eval/clr_summary.txt"
      # practicality: "prompt/template/g_eval/prc_summary.txt"

# Report 평가 관련 설정
report:
  metrics:
    - g-eval

  g_eval:
    openai_model: "gpt-4o" # report는 gpt-4o로도 가능
    additional: False # "readability", "clearance", "practicality"를 G-Eval에 적용할 여부
    prompts:
      consistency: "prompt/template/g_eval/con_report.txt"
      coherence: "prompt/template/g_eval/coh_report.txt"
      fluency: "prompt/template/g_eval/flu_report.txt"
      relevance: "prompt/template/g_eval/rel_report.txt"
      # readability: "prompt/template/g_eval/rdb_report.txt"
      # clearance: "prompt/template/g_eval/clr_report.txt"
      # practicality: "prompt/template/g_eval/prc_report.txt"

# Classification 평가 관련 설정 (추후 추가)
classification:
  do_manual_filter: False
  # Consistency 평가 용 반복 추론 횟수 설정
  inference: 5

self_refine:
  max_iteration: 5

embedding:
  model_name: "bge-m3" # 혹은 "upstage"
  similarity_metric: "cosine-similarity" # 혹은 "dot-product"
  similarity_threshold: 0.8
  save_results: true

token_tracking: true
